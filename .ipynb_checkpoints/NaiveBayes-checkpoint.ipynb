{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.数据导入与划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession # SparkSession 是Spark 2.0版本的新入口\n",
    "spark = SparkSession.builder.master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = spark.read.csv(path=\"hdfs://localhost:9000/user/bdlab/lab2/SUSY.csv.gz\",header=False,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = raw_data.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=0.0, count=2712173), Row(_c0=1.0, count=2287827)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.groupBy(raw_data[0]).count().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0/1 基本概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_0 = 2712173/5000000\n",
    "prob_1 = 1-prob_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=0.0, count=2170505), Row(_c0=1.0, count=1829609)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练集0/1分布\n",
    "train.groupBy(raw_data[0]).count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=0.0, count=541668), Row(_c0=1.0, count=458218)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试集0/1分布\n",
    "test.groupBy(raw_data[0]).count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分后写入hdfs\n",
    "test.write.csv(path=\"hdfs://localhost:9000/user/bdlab/lab2/SUSY_test.csv.gzip\",compression=\"gzip\")\n",
    "train.write.csv(path=\"hdfs://localhost:9000/user/bdlab/lab2/SUSY_train.csv.gzip\",compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载train\n",
    "train = spark.read.csv(path=\"hdfs://localhost:9000/user/bdlab/lab2/SUSY_train.csv.gzip\",header=False,inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test.select(test.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = test.select(test.columns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_mean_func = { _:\"mean\" for _ in train.columns[1:] }\n",
    "cal_var_func = { _:\"variance\" for _ in train.columns[1:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train = train.groupBy(train[0]).agg(cal_mean_func).collect()\n",
    "var_train = train.groupBy(train[0]).agg(cal_var_func).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_0 = [ mean_train[0][\"avg(_c\"+str(i)+str(\")\")] for i in range(1,19)]\n",
    "mean_1 = [ mean_train[1][\"avg(_c\"+str(i)+str(\")\")] for i in range(1,19)]\n",
    "var_0 = [ var_train[0][\"variance(_c\"+str(i)+str(\")\")] for i in range(1,19)]\n",
    "var_1 = [ var_train[0][\"variance(_c\"+str(i)+str(\")\")] for i in range(1,19)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存统计信息\n",
    "np.savetxt(\"nb_sta\",[mean_0,mean_1,var_0,var_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取统计信息\n",
    "mean_0,mean_1,var_0,var_1 = np.loadtxt(\"nb_sta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_pi = 1/ np.sqrt(2*np.pi)\n",
    "def GaussianProb(x,miu,sigmaq):\n",
    "    left = const_pi / np.sqrt(sigmaq)\n",
    "    right = np.exp(- (x - miu)**2 / (2 * sigmaq))\n",
    "    return left*right\n",
    "\n",
    "def NB_classifier(x):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        x: a vec with len of 18\n",
    "    ouput:\n",
    "        result: 0 or 1\n",
    "    \"\"\"\n",
    "    if len(x) != 18:\n",
    "        raise ValueError\n",
    "        \n",
    "    # 计算 0 概率\n",
    "    result_0 = prob_0\n",
    "    for i in range(18):\n",
    "        result_0 = result_0 * GaussianProb(x[i],mean_0[i],var_0[i])\n",
    "        \n",
    "    # 计算 1 概率\n",
    "    result_1 = prob_1\n",
    "    for i in range(18):\n",
    "        result_1 = result_1 * GaussianProb(x[i],mean_1[i],var_1[i])\n",
    "    return 0 if result_0>result_1 else 1\n",
    "\n",
    "def NB_predict(testset):\n",
    "    return [ NB_classifier(_) for _ in testset ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,recall_score,classification_report\n",
    "\n",
    "def metric(true,pre):\n",
    "    print(\"Accuracy: {:.4f}, Recall: {:.4f} \".format(accuracy_score(true,pre),recall_score(true,pre)))\n",
    "    print()\n",
    "    print(classification_report(true,pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"test_data_arr\",test_data_arr)\n",
    "np.save(\"test_label_arr\",test_label_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_arr = np.load(\"test_data_arr.npy\")\n",
    "test_label_arr = np.load(\"test_label_arr.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pre_nb = NB_predict(test_data_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7360, Recall: 0.6047 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.72      0.85      0.78    541668\n",
      "        1.0       0.77      0.60      0.68    458218\n",
      "\n",
      "avg / total       0.74      0.74      0.73    999886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metric(test_label_arr,test_pre_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Logistic-Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train = train.sample(0.01)\n",
    "sample_train_list = sample_train.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "testrow = sample_train_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"sample_train_list\",sample_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_list = np.load(\"sample_train_list.npy\")\n",
    "sample_train = spark.sparkContext.parallelize(sample_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n 特征数\n",
    "n = 18\n",
    "theta = np.zeros(n+1) # 在LR_fit 中更新\n",
    "\n",
    "def sigmod(x):\n",
    "    return 1/(1+np.exp( - np.matmul(x,theta)))\n",
    "\n",
    "# 对数损失函数\n",
    "def cost_mapper(x):\n",
    "    h = sigmod(x[0])\n",
    "    L = 0\n",
    "    if x[1]==0:\n",
    "        L = np.log(1-h)\n",
    "    else:\n",
    "        L = np.log(h)\n",
    "    return -L \n",
    "\n",
    "# 计算梯度 (h(x)-y)*x\n",
    "def grad_mapper(x):\n",
    "    return (sigmod(x[0]) - x[1])*x[0]\n",
    "\n",
    "def sum_reducer(x,y):\n",
    "    # (grad,cost)\n",
    "    return (x[0]+y[0],x[1]+y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def LR_fit(data,max_iter=30,alpha=0.1,tol=0.001,penalty=0.1):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        data: rdd [x0,x1,...,xn], x0为label\n",
    "        max_iter : 最大迭代次数\n",
    "        alpha : 学习率\n",
    "        tol: 收敛容忍度\n",
    "        penalty: 惩罚系数\n",
    "    output:\n",
    "        theta: \n",
    "    \"\"\"\n",
    "    iter_cnt = 0\n",
    "    m = data.count() # 样本数\n",
    "    # x => ((1,x1,x2,...,xn),y)\n",
    "    data = data.map(lambda x:(np.asarray((1,)+x[1:]),x[0]) )\n",
    "    cost_pre = 0\n",
    "    time_start = time.time()\n",
    "    while iter_cnt<max_iter:\n",
    "        grad,cost = data.map(lambda x:(grad_mapper(x),cost_mapper(x))).reduce(sum_reducer)\n",
    "        # 计算代价\n",
    "        cost = cost/m + penalty*np.sum(np.square(theta))/(2*m)\n",
    "        # 更新 Θ\n",
    "        theta[0] = theta[0] - alpha*grad[0] /m\n",
    "        theta[1:] = theta[1:] - alpha*(grad[1:]+penalty*theta[1:]) /m\n",
    "        \n",
    "        cost_del = cost_pre - cost\n",
    "        cost_del = -cost_del if cost_pre==0 else cost_del \n",
    "        time_end = time.time()\n",
    "        print(\"iter {}, cost={:.8f}, △cost={:.8f}, △time={:.2f}\".format(iter_cnt,cost,cost_del,time_end-time_start))\n",
    "        \n",
    "        if cost_del<tol:\n",
    "            print(\"收敛,算法结束\")\n",
    "        cost_pre = cost\n",
    "        time_start = time_end\n",
    "        iter_cnt +=1\n",
    "        \n",
    "    print(\"迭代上限,算法结束\")\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分类器\n",
    "def LR_classifier(x):\n",
    "    h = sigmod(np.insert(x,0,1))\n",
    "    return 0 if h<0.5 else 1\n",
    "\n",
    "def LR_predict(testset):\n",
    "    return [ LR_classifier(_) for _ in testset ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, cost=0.6931471805593518, △cost=0.6931471805593518, △time=52.38\n",
      "iter 1, cost=0.6882139385136494, △cost=0.004933242045702335, △time=53.38\n",
      "iter 2, cost=0.6841936322932476, △cost=0.004020306220401837, △time=52.25\n",
      "iter 3, cost=0.6806242263182065, △cost=0.0035694059750410734, △time=54.65\n",
      "iter 4, cost=0.6773068522325246, △cost=0.00331737408568189, △time=54.45\n",
      "iter 5, cost=0.6741542321075942, △cost=0.0031526201249304497, △time=52.48\n",
      "iter 6, cost=0.6711263829311682, △cost=0.0030278491764259563, △time=53.21\n",
      "iter 7, cost=0.6682034940627589, △cost=0.0029228888684093013, △time=52.35\n",
      "iter 8, cost=0.6653744769248472, △cost=0.002829017137911727, △time=52.59\n",
      "iter 9, cost=0.6626321154981288, △cost=0.002742361426718354, △time=52.37\n",
      "iter 10, cost=0.6599710022961874, △cost=0.0026611132019414008, △time=51.16\n",
      "iter 11, cost=0.6573866527245563, △cost=0.002584349571631117, △time=51.38\n",
      "iter 12, cost=0.6548751190997462, △cost=0.0025115336248101583, △time=51.96\n",
      "iter 13, cost=0.6524328169981621, △cost=0.0024423021015840263, △time=63.27\n",
      "iter 14, cost=0.6500564421055514, △cost=0.0023763748926107775, △time=58.20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-2a4b2f5ce206>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLR_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-a226fb95bf7b>\u001b[0m in \u001b[0;36mLR_fit\u001b[0;34m(data, max_iter, alpha, tol, penalty)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtime_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0miter_cnt\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_mapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost_mapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_reducer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m# 计算代价\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \"\"\"\n\u001b[1;32m    813\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "theta = LR_fit(data=sample_train.rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09697159,  0.08209373, -0.00150128, -0.002786  , -0.01411221,\n",
       "        0.00256429,  0.00105199,  0.15344194, -0.00166682,  0.0619158 ,\n",
       "        0.04945456,  0.01390256,  0.06590398, -0.06362982, -0.05282876,\n",
       "        0.0079296 ,  0.01298479, -0.09001692,  0.01508648])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pre_lr = LR_predict(test_data_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7379, Recall: 0.8077 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.81      0.68      0.74    541668\n",
      "        1.0       0.68      0.81      0.74    458218\n",
      "\n",
      "avg / total       0.75      0.74      0.74    999886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metric(test_label_arr,test_pre_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.array 关于向量乘数值的运算速度测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.5 ms, sys: 16.6 ms, total: 32.2 ms\n",
      "Wall time: 52.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def grad_mapper(x):\n",
    "    return (sigmod(x[0]) - x[1])*x[0]\n",
    "grad,cost = sample_train.rdd.map(lambda x:(np.asarray((1,)+x[1:]),x[0]) ).map(lambda x:(grad_mapper(x),cost_mapper(x))).reduce(sum_reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 231 ms, sys: 78.7 ms, total: 309 ms\n",
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def grad_mapper(x):\n",
    "    h = (sigmod(x[0]) - x[1])\n",
    "    return [ h*_ for _ in x[0]]\n",
    "grad,cost = sample_train.rdd.map(lambda x:(np.asarray((1,)+x[1:]),x[0]) ).map(lambda x:(grad_mapper(x),cost_mapper(x))).reduce(sum_reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 204 ms, sys: 41.4 ms, total: 246 ms\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def grad_mapper(x):\n",
    "    h = (sigmod(x[0]) - x[1])\n",
    "    return [ h*_ for _ in x[0]]\n",
    "grad,cost = sample_train.rdd.map(lambda x:((1,)+x[1:],x[0]) ).map(lambda x:(grad_mapper(x),cost_mapper(x))).reduce(sum_reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40230"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
