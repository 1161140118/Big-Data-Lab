{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.数据导入与划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession # SparkSession 是Spark 2.0版本的新入口\n",
    "spark = SparkSession.builder.master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = spark.read.csv(path=\"hdfs://localhost:9000/user/bdlab/lab2/SUSY.csv.gz\",header=False,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = raw_data.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=0.0, count=2712173), Row(_c0=1.0, count=2287827)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.groupBy(raw_data[0]).count().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0/1 基本概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_0 = 2712173/5000000\n",
    "prob_1 = 1-prob_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=0.0, count=2170505), Row(_c0=1.0, count=1829609)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练集0/1分布\n",
    "train.groupBy(raw_data[0]).count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=0.0, count=541668), Row(_c0=1.0, count=458218)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试集0/1分布\n",
    "test.groupBy(raw_data[0]).count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分后写入hdfs\n",
    "test.write.csv(path=\"hdfs://localhost:9000/user/bdlab/lab2/SUSY_test.csv.gzip\",compression=\"gzip\")\n",
    "train.write.csv(path=\"hdfs://localhost:9000/user/bdlab/lab2/SUSY_train.csv.gzip\",compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载train\n",
    "train = spark.read.csv(path=\"hdfs://localhost:9000/user/bdlab/lab2/SUSY_train.csv.gzip\",header=False,inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test.select(test.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = test.select(test.columns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_mean_func = { _:\"mean\" for _ in train.columns[1:] }\n",
    "cal_var_func = { _:\"variance\" for _ in train.columns[1:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train = train.groupBy(train[0]).agg(cal_mean_func).collect()\n",
    "var_train = train.groupBy(train[0]).agg(cal_var_func).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_0 = [ mean_train[0][\"avg(_c\"+str(i)+str(\")\")] for i in range(1,19)]\n",
    "mean_1 = [ mean_train[1][\"avg(_c\"+str(i)+str(\")\")] for i in range(1,19)]\n",
    "var_0 = [ var_train[0][\"variance(_c\"+str(i)+str(\")\")] for i in range(1,19)]\n",
    "var_1 = [ var_train[0][\"variance(_c\"+str(i)+str(\")\")] for i in range(1,19)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存统计信息\n",
    "np.savetxt(\"nb_sta\",[mean_0,mean_1,var_0,var_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取统计信息\n",
    "mean_0,mean_1,var_0,var_1 = np.loadtxt(\"nb_sta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_pi = 1/ np.sqrt(2*np.pi)\n",
    "def GaussianProb(x,miu,sigmaq):\n",
    "    left = const_pi / np.sqrt(sigmaq)\n",
    "    right = np.exp(- (x - miu)**2 / (2 * sigmaq))\n",
    "    return left*right\n",
    "\n",
    "def NB_classifier(x):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        x: a vec with len of 18\n",
    "    ouput:\n",
    "        result: 0 or 1\n",
    "    \"\"\"\n",
    "    if len(x) != 18:\n",
    "        raise ValueError\n",
    "        \n",
    "    # 计算 0 概率\n",
    "    result_0 = prob_0\n",
    "    for i in range(18):\n",
    "        result_0 = result_0 * GaussianProb(x[i],mean_0[i],var_0[i])\n",
    "        \n",
    "    # 计算 1 概率\n",
    "    result_1 = prob_1\n",
    "    for i in range(18):\n",
    "        result_1 = result_1 * GaussianProb(x[i],mean_1[i],var_1[i])\n",
    "    return 0 if result_0>result_1 else 1\n",
    "\n",
    "def NB_predict(testset):\n",
    "    return [ NB_classifier(_) for _ in testset ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,recall_score,classification_report\n",
    "\n",
    "def metric(true,pre):\n",
    "    print(\"Accuracy: {:.4f}, Recall: {:.4f} \".format(accuracy_score(true,pre),recall_score(true,pre)))\n",
    "    print()\n",
    "    print(classification_report(true,pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"test_data_arr\",test_data_arr)\n",
    "np.save(\"test_label_arr\",test_label_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_arr = np.load(\"test_data_arr.npy\")\n",
    "test_label_arr = np.load(\"test_label_arr.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pre_nb = NB_predict(test_data_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7360, Recall: 0.6047 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.72      0.85      0.78    541668\n",
      "        1.0       0.77      0.60      0.68    458218\n",
      "\n",
      "avg / total       0.74      0.74      0.73    999886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metric(test_label_arr,test_pre_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Logistic-Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train = train.sample(0.001)\n",
    "sample_train_list = sample_train.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "testrow = sample_train_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"sample_train_list\",sample_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n 特征数\n",
    "n = 18\n",
    "theta = np.zeros(n+1) # 在LR_fit 中更新\n",
    "\n",
    "def sigmod(x):\n",
    "    return 1/(1+np.exp( - np.matmul(x,theta)))\n",
    "\n",
    "# 对数损失函数\n",
    "def cost_mapper(x):\n",
    "    h = sigmod(x[0])\n",
    "    L = 0\n",
    "    if x[1]==0:\n",
    "        L = np.log(1-h)\n",
    "    else:\n",
    "        L = np.log(h)\n",
    "    return -L \n",
    "\n",
    "# 计算梯度 (h(x)-y)*x\n",
    "def grad_mapper(x):\n",
    "    return (sigmod(x[0]) - x[1])*x[0]\n",
    "\n",
    "def sum_reducer(x,y):\n",
    "    # (grad,cost)\n",
    "    return (x[0]+y[0],x[1]+y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-7-958d5be8c9bd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-958d5be8c9bd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def LR_fit():\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def LR_fit(data,max_iter=30,alpha=0.1,tol=0.0001,penalty=0.1):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        data: rdd [x0,x1,...,xn], x0为label\n",
    "        max_iter : 最大迭代次数\n",
    "        alpha : 学习率\n",
    "        tol: 收敛容忍度\n",
    "        penalty: 惩罚系数\n",
    "    \"\"\"\n",
    "    iter_cnt = 0\n",
    "    m = data.count() # 样本数\n",
    "    # x => ((1,x1,x2,...,xn),y)\n",
    "    data = data.map(lambda x:(np.asarray((1,)+x[1:]),x[0]) )\n",
    "    cost_pre = 0\n",
    "    while iter_cnt<max_iter:\n",
    "        grad,cost = data.map(lambda x:(grad_mapper(x),cost_mapper(x))).reduce(sum_reducer).collect()\n",
    "        theta[0] = theta[0] - alpha/m * grad[0]\n",
    "        theta[1:] = theta[1:] - alpha/m * (grad[1:]+penalty*theta[1:]\n",
    "        \n",
    "                                           \n",
    "        iter_cnt +=1\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import  LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
